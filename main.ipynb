{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import cm as CM\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def image_to_matrix(path):\n",
    "    img = cv2.imread(path,0)\n",
    "    img = img.astype(np.float32, copy=False)\n",
    "    ht = img.shape[0]\n",
    "    wd = img.shape[1]\n",
    "    ht_1 = int((ht/4)*4)\n",
    "    wd_1 = int((wd/4)*4)\n",
    "    img = cv2.resize(img,(wd_1,ht_1))\n",
    "    img = img.reshape((1,1,img.shape[0],img.shape[1]))\n",
    "    v = Variable(torch.from_numpy(img).type(torch.FloatTensor))\n",
    "    v = v.to('cuda')\n",
    "\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_density_map_with_fixed_kernel(img,points,kernel_size=25,sigma=6.0):\n",
    "\n",
    "    def guassian_kernel(size,sigma):\n",
    "        rows=size[0] \n",
    "        cols=size[1]\n",
    "        mean_x=int((rows-1)/2)\n",
    "        mean_y=int((cols-1)/2)\n",
    "\n",
    "        f=np.zeros(size)\n",
    "        for x in range(0,rows):\n",
    "            for y in range(0,cols):\n",
    "                mean_x2=(x-mean_x)*(x-mean_x)\n",
    "                mean_y2=(y-mean_y)*(y-mean_y)\n",
    "                f[x,y]=(1.0/(2.0*np.pi*sigma*sigma))*np.exp((mean_x2+mean_y2)/(-2.0*sigma*sigma))\n",
    "        return f\n",
    "\n",
    "    [rows,cols]=[img.shape[0],img.shape[1]]\n",
    "    d_map=np.zeros([rows,cols])\n",
    "    f=guassian_kernel([kernel_size,kernel_size],sigma) # generate gaussian kernel with fixed size.\n",
    "    normed_f=(1.0/f.sum())*f # normalization for each head.\n",
    "\n",
    "    if len(points)==0:\n",
    "        return d_map\n",
    "    else:\n",
    "        for p in points:\n",
    "            r,c=int(p[0]),int(p[1])\n",
    "            if r>=rows or c>=cols:\n",
    "                continue\n",
    "            for x in range(0,f.shape[0]):\n",
    "                for y in range(0,f.shape[1]):\n",
    "                    if x+((r+1)-int((f.shape[0]-1)/2))<0 or x+((r+1)-int((f.shape[0]-1)/2))>rows-1 \\\n",
    "                    or y+((c+1)-int((f.shape[1]-1)/2))<0 or y+((c+1)-int((f.shape[1]-1)/2))>cols-1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        d_map[x+((r+1)-int((f.shape[0]-1)/2)),y+((c+1)-int((f.shape[1]-1)/2))]+=normed_f[x,y]\n",
    "    return d_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def density_map(img_path,mat_path):\n",
    "    img=plt.imread(img_path)\n",
    "    mat = scipy.io.loadmat(mat_path)\n",
    "    pts = mat[\"image_info\"][0,0][0,0][0] \n",
    "    points=[]\n",
    "    for p in pts:\n",
    "        points.append([p[1],p[0]]) \n",
    "    density_map=generate_density_map_with_fixed_kernel(img,points)\n",
    "    return density_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, relu=True, same_padding=False, bn=False):\n",
    "        super(Conv2d, self).__init__()\n",
    "        padding = int((kernel_size - 1) / 2) if same_padding else 0\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU(inplace=True) if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MCNN(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, bn=False):\n",
    "        super(MCNN, self).__init__()\n",
    "        \n",
    "        self.branch1 = nn.Sequential(Conv2d( 1, 16, 9, same_padding=True, bn=bn),\n",
    "                                     nn.MaxPool2d(2),\n",
    "                                     Conv2d(16, 32, 7, same_padding=True, bn=bn),\n",
    "                                     nn.MaxPool2d(2),\n",
    "                                     Conv2d(32, 16, 7, same_padding=True, bn=bn),\n",
    "                                     Conv2d(16,  8, 7, same_padding=True, bn=bn))\n",
    "        \n",
    "        self.branch2 = nn.Sequential(Conv2d( 1, 20, 7, same_padding=True, bn=bn),\n",
    "                                     nn.MaxPool2d(2),\n",
    "                                     Conv2d(20, 40, 5, same_padding=True, bn=bn),\n",
    "                                     nn.MaxPool2d(2),\n",
    "                                     Conv2d(40, 20, 5, same_padding=True, bn=bn),\n",
    "                                     Conv2d(20, 10, 5, same_padding=True, bn=bn))\n",
    "        \n",
    "        self.branch3 = nn.Sequential(Conv2d( 1, 24, 5, same_padding=True, bn=bn),\n",
    "                                     nn.MaxPool2d(2),\n",
    "                                     Conv2d(24, 48, 3, same_padding=True, bn=bn),\n",
    "                                     nn.MaxPool2d(2),\n",
    "                                     Conv2d(48, 24, 3, same_padding=True, bn=bn),\n",
    "                                     Conv2d(24, 12, 3, same_padding=True, bn=bn))\n",
    "        \n",
    "        self.fuse = nn.Sequential(Conv2d( 30, 1, 1, same_padding=True, bn=bn))\n",
    "        \n",
    "    def forward(self, im_data):\n",
    "        x1 = self.branch1(im_data)\n",
    "        x2 = self.branch2(im_data)\n",
    "        x3 = self.branch3(im_data)\n",
    "        x = torch.cat((x1,x2,x3),1)\n",
    "        x = self.fuse(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        # Flatten the matrices to vectors\n",
    "        predicted_flat = predicted.view(-1)\n",
    "        target_flat = target.view(-1)\n",
    "        \n",
    "        # Calculate the MSE loss\n",
    "        mse_loss = torch.mean((predicted_flat - target_flat) ** 2)\n",
    "        \n",
    "        return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Branch1(nn.Module):\n",
    "    def __init__(self, bn=False):\n",
    "        super(Branch1, self).__init__()\n",
    "        self.conv1 = Conv2d(1, 16, 9, same_padding=True, bn=bn)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = Conv2d(16, 32, 7, same_padding=True, bn=bn)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = Conv2d(32, 16, 7, same_padding=True, bn=bn)\n",
    "        self.conv4 = Conv2d(16, 8, 7, same_padding=True, bn=bn)\n",
    "        #self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.mean(x, dim=1, keepdim=False)\n",
    "        x = x.squeeze()\n",
    "\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        return x\n",
    "\n",
    "class Branch2(nn.Module):\n",
    "    def __init__(self, bn=False):\n",
    "        super(Branch2, self).__init__()\n",
    "        self.conv1 = Conv2d(1, 20, 7, same_padding=True, bn=bn)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = Conv2d(20, 40, 5, same_padding=True, bn=bn)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = Conv2d(40, 20, 5, same_padding=True, bn=bn)\n",
    "        self.conv4 = Conv2d(20, 10, 5, same_padding=True, bn=bn)\n",
    "        #self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.mean(x, dim=1, keepdim=False)\n",
    "        x = x.squeeze()\n",
    "\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        return x\n",
    "    \n",
    "class Branch3(nn.Module):\n",
    "    def __init__(self, bn=False):\n",
    "        super(Branch3, self).__init__()\n",
    "        self.conv1 = Conv2d(1, 24, 5, same_padding=True, bn=bn)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = Conv2d(24, 48, 3, same_padding=True, bn=bn)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = Conv2d(48, 24, 3, same_padding=True, bn=bn)\n",
    "        self.conv4 = Conv2d(24, 12, 3, same_padding=True, bn=bn)\n",
    "        #self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.mean(x, dim=1, keepdim=False)\n",
    "        x = x.squeeze()\n",
    "\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "parent_directory = os.path.join(parent_directory,'part_A')\n",
    "parent_directory = os.path.join(parent_directory,'train_data')\n",
    "parent_directory = os.path.join(parent_directory,'images')\n",
    "parent_directory\n",
    "\n",
    "files = os.listdir(parent_directory)\n",
    "directories = []\n",
    "for file in files:\n",
    "    directories.append(os.path.join(parent_directory,file))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "parent_directory = os.path.join(parent_directory,'part_A')\n",
    "parent_directory = os.path.join(parent_directory,'train_data')\n",
    "parent_directory = os.path.join(parent_directory,'ground-truth')\n",
    "parent_directory\n",
    "\n",
    "files = os.listdir(parent_directory)\n",
    "gt_directories = []\n",
    "for file in files:\n",
    "    gt_directories.append(os.path.join(parent_directory,file))\n",
    "\n",
    "gt_directories.sort()\n",
    "directories.sort()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 700\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "parent_directory = os.path.join(parent_directory,'part_B')\n",
    "parent_directory = os.path.join(parent_directory,'train_data')\n",
    "parent_directory = os.path.join(parent_directory,'images')\n",
    "parent_directory\n",
    "\n",
    "files = os.listdir(parent_directory)\n",
    "\n",
    "for file in files:\n",
    "    directories.append(os.path.join(parent_directory,file))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "parent_directory = os.path.join(parent_directory,'part_B')\n",
    "parent_directory = os.path.join(parent_directory,'train_data')\n",
    "parent_directory = os.path.join(parent_directory,'ground-truth')\n",
    "parent_directory\n",
    "\n",
    "files = os.listdir(parent_directory)\n",
    "\n",
    "for file in files:\n",
    "    gt_directories.append(os.path.join(parent_directory,file))\n",
    "\n",
    "gt_directories.sort()\n",
    "directories.sort()\n",
    "\n",
    "print(len(directories),len(gt_directories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "for i in range(len(directories)):\n",
    "    # Assuming density_map function generates a tensor\n",
    "    tensor = density_map(directories[i], gt_directories[i])\n",
    "    h,w = tensor.shape[0],tensor.shape[1]\n",
    "    \n",
    "    tensor =  cv2.resize(tensor,(tensor.shape[0]//4,tensor.shape[1]//4))\n",
    "    h1,w1 = tensor.shape[0],tensor.shape[1]\n",
    "    tensor = tensor*(h*w/(h1*w1))\n",
    "    \n",
    "    tensor = torch.Tensor(tensor)\n",
    "    tensor = tensor.to('cuda')  # Move tensor to GPU\n",
    "    images.append(tensor)\n",
    "    print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = []\n",
    "\n",
    "for i in range(len(directories)):\n",
    "\n",
    "    image = image_to_matrix(directories[i])\n",
    "\n",
    "    inputs.append(image.to('cuda'))\n",
    "    print(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768, 1024]) torch.Size([256, 192])\n",
      "1544.4252 0.2857586\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0].shape,images[0].shape)\n",
    "\n",
    "\n",
    "#print(np.sum(images[0].cpu().numpy()),np.sum(model(inputs[0]).detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Branch1(\n",
       "  (conv1): Conv2d(\n",
       "    (conv): Conv2d(1, 16, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(\n",
       "    (conv): Conv2d(16, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(\n",
       "    (conv): Conv2d(32, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv4): Conv2d(\n",
       "    (conv): Conv2d(16, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0].device,images[0].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.022252654656767845\n",
      "Epoch [2/20], Loss: 0.006718245800584555\n",
      "Epoch [3/20], Loss: 0.003776242956519127\n",
      "Epoch [4/20], Loss: 0.002291240496560931\n",
      "Epoch [5/20], Loss: 0.00149337702896446\n",
      "Epoch [6/20], Loss: 0.0011032327311113477\n",
      "Epoch [7/20], Loss: 0.0009352268534712493\n",
      "Epoch [8/20], Loss: 0.0008785458630882204\n",
      "Epoch [9/20], Loss: 0.0008539087139070034\n",
      "Epoch [10/20], Loss: 0.0008418465731665492\n",
      "Epoch [11/20], Loss: 0.0008348003029823303\n",
      "Epoch [12/20], Loss: 0.0008301417692564428\n",
      "Epoch [13/20], Loss: 0.000826955190859735\n",
      "Epoch [14/20], Loss: 0.0008247548248618841\n",
      "Epoch [15/20], Loss: 0.0008232865948230028\n",
      "Epoch [16/20], Loss: 0.0008224344346672297\n",
      "Epoch [17/20], Loss: 0.0008219570736400783\n",
      "Epoch [18/20], Loss: 0.0008216702844947577\n",
      "Epoch [19/20], Loss: 0.0008214914705604315\n",
      "Epoch [20/20], Loss: 0.0008213941473513842\n"
     ]
    }
   ],
   "source": [
    "# Define your loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define your optimizer\n",
    "model = Branch1()\n",
    "model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "# Assuming 'inputs' is your list of inputs and 'images' is your list of target images\n",
    "# Assuming both 'inputs' and 'images' are PyTorch tensors or numpy arrays\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'inputs' and 'images' have the same length\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to train mode\n",
    "    \n",
    "\n",
    "    for i in range(len(images)):\n",
    "        try:\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "            # Forward pass\n",
    "            output_image = model(inputs[i])\n",
    "\n",
    "            loss = criterion(output_image, images[i])\n",
    "\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n",
    "    \n",
    "           \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'branch1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.00038446931284852326\n",
      "Epoch [2/200], Loss: 0.0003497058351058513\n",
      "Epoch [3/200], Loss: 0.0003171300340909511\n",
      "Epoch [4/200], Loss: 0.0002867973526008427\n",
      "Epoch [5/200], Loss: 0.0002581972803454846\n",
      "Epoch [6/200], Loss: 0.00023132991918828338\n",
      "Epoch [7/200], Loss: 0.00020636102999560535\n",
      "Epoch [8/200], Loss: 0.0001832045818446204\n",
      "Epoch [9/200], Loss: 0.00016201961261685938\n",
      "Epoch [10/200], Loss: 0.0001428413816029206\n",
      "Epoch [11/200], Loss: 0.00012562971096485853\n",
      "Epoch [12/200], Loss: 0.00011021898535545915\n",
      "Epoch [13/200], Loss: 9.682885865913704e-05\n",
      "Epoch [14/200], Loss: 8.515013905707747e-05\n",
      "Epoch [15/200], Loss: 7.518001075368375e-05\n",
      "Epoch [16/200], Loss: 6.672156450804323e-05\n",
      "Epoch [17/200], Loss: 5.96054014749825e-05\n",
      "Epoch [18/200], Loss: 5.3590250900015235e-05\n",
      "Epoch [19/200], Loss: 4.840227484237403e-05\n",
      "Epoch [20/200], Loss: 4.3838514102390036e-05\n",
      "Epoch [21/200], Loss: 3.978559107054025e-05\n",
      "Epoch [22/200], Loss: 3.612664659158327e-05\n",
      "Epoch [23/200], Loss: 3.2858908525668085e-05\n",
      "Epoch [24/200], Loss: 2.9887001801398583e-05\n",
      "Epoch [25/200], Loss: 2.7224990844842978e-05\n",
      "Epoch [26/200], Loss: 2.482331001374405e-05\n",
      "Epoch [27/200], Loss: 2.267045965709258e-05\n",
      "Epoch [28/200], Loss: 2.0690993551397696e-05\n",
      "Epoch [29/200], Loss: 1.8893222659244202e-05\n",
      "Epoch [30/200], Loss: 1.727066955936607e-05\n",
      "Epoch [31/200], Loss: 1.578826231707353e-05\n",
      "Epoch [32/200], Loss: 1.4452058167080395e-05\n",
      "Epoch [33/200], Loss: 1.3241241504147183e-05\n",
      "Epoch [34/200], Loss: 1.2155617696407717e-05\n",
      "Epoch [35/200], Loss: 1.1176200132467784e-05\n",
      "Epoch [36/200], Loss: 1.0307559023203794e-05\n",
      "Epoch [37/200], Loss: 9.541356121189892e-06\n",
      "Epoch [38/200], Loss: 8.85972985997796e-06\n",
      "Epoch [39/200], Loss: 8.26197265269002e-06\n",
      "Epoch [40/200], Loss: 7.730872312095016e-06\n",
      "Epoch [41/200], Loss: 7.2661541707930155e-06\n",
      "Epoch [42/200], Loss: 6.852571914350847e-06\n",
      "Epoch [43/200], Loss: 6.484977802756475e-06\n",
      "Epoch [44/200], Loss: 6.159327767818468e-06\n",
      "Epoch [45/200], Loss: 5.868219432159094e-06\n",
      "Epoch [46/200], Loss: 5.604098078038078e-06\n",
      "Epoch [47/200], Loss: 5.3690791901317425e-06\n",
      "Epoch [48/200], Loss: 5.153905931365443e-06\n",
      "Epoch [49/200], Loss: 4.9606851462158374e-06\n",
      "Epoch [50/200], Loss: 4.787841135112103e-06\n",
      "Epoch [51/200], Loss: 4.6311633923323825e-06\n",
      "Epoch [52/200], Loss: 4.4894954953633714e-06\n",
      "Epoch [53/200], Loss: 4.360946149972733e-06\n",
      "Epoch [54/200], Loss: 4.247446668159682e-06\n",
      "Epoch [55/200], Loss: 4.143849309912184e-06\n",
      "Epoch [56/200], Loss: 4.052132680953946e-06\n",
      "Epoch [57/200], Loss: 3.9702617868897505e-06\n",
      "Epoch [58/200], Loss: 3.896946964232484e-06\n",
      "Epoch [59/200], Loss: 3.830906280199997e-06\n",
      "Epoch [60/200], Loss: 3.7735301248176256e-06\n",
      "Epoch [61/200], Loss: 3.7230533962429035e-06\n",
      "Epoch [62/200], Loss: 3.676531605378841e-06\n",
      "Epoch [63/200], Loss: 3.636897872638656e-06\n",
      "Epoch [64/200], Loss: 3.6025048757437617e-06\n",
      "Epoch [65/200], Loss: 3.569004320524982e-06\n",
      "Epoch [66/200], Loss: 3.539810904840124e-06\n",
      "Epoch [67/200], Loss: 3.5149128052580636e-06\n",
      "Epoch [68/200], Loss: 3.4883566968346713e-06\n",
      "Epoch [69/200], Loss: 3.465381269052159e-06\n",
      "Epoch [70/200], Loss: 3.4425022477080347e-06\n",
      "Epoch [71/200], Loss: 3.4228196454932913e-06\n",
      "Epoch [72/200], Loss: 3.4016036352113588e-06\n",
      "Epoch [73/200], Loss: 3.3838227864180226e-06\n",
      "Epoch [74/200], Loss: 3.3665189675957663e-06\n",
      "Epoch [75/200], Loss: 3.34988976646855e-06\n",
      "Epoch [76/200], Loss: 3.3354499464621767e-06\n",
      "Epoch [77/200], Loss: 3.316569063827046e-06\n",
      "Epoch [78/200], Loss: 3.30222269440128e-06\n",
      "Epoch [79/200], Loss: 3.2869722872419516e-06\n",
      "Epoch [80/200], Loss: 3.273126822023187e-06\n",
      "Epoch [81/200], Loss: 3.262211066612508e-06\n",
      "Epoch [82/200], Loss: 3.248794200771954e-06\n",
      "Epoch [83/200], Loss: 3.2358836961066118e-06\n",
      "Epoch [84/200], Loss: 3.2214086331805447e-06\n",
      "Epoch [85/200], Loss: 3.209599753972725e-06\n",
      "Epoch [86/200], Loss: 3.197267915311386e-06\n",
      "Epoch [87/200], Loss: 3.188300297551905e-06\n",
      "Epoch [88/200], Loss: 3.1770850910106674e-06\n",
      "Epoch [89/200], Loss: 3.1670572298025945e-06\n",
      "Epoch [90/200], Loss: 3.157318815283361e-06\n",
      "Epoch [91/200], Loss: 3.1480817597184796e-06\n",
      "Epoch [92/200], Loss: 3.1379756819660543e-06\n",
      "Epoch [93/200], Loss: 3.1310289614339126e-06\n",
      "Epoch [94/200], Loss: 3.1210620363708586e-06\n",
      "Epoch [95/200], Loss: 3.1130177831073524e-06\n",
      "Epoch [96/200], Loss: 3.1052347821969306e-06\n",
      "Epoch [97/200], Loss: 3.097761691606138e-06\n",
      "Epoch [98/200], Loss: 3.090169002462062e-06\n",
      "Epoch [99/200], Loss: 3.0834232802590122e-06\n",
      "Epoch [100/200], Loss: 3.076823531955597e-06\n",
      "Epoch [101/200], Loss: 3.0705441531608813e-06\n",
      "Epoch [102/200], Loss: 3.062034011236392e-06\n",
      "Epoch [103/200], Loss: 3.0575163236790104e-06\n",
      "Epoch [104/200], Loss: 3.051204885196057e-06\n",
      "Epoch [105/200], Loss: 3.044562618015334e-06\n",
      "Epoch [106/200], Loss: 3.039287548745051e-06\n",
      "Epoch [107/200], Loss: 3.033854454770335e-06\n",
      "Epoch [108/200], Loss: 3.028990704478929e-06\n",
      "Epoch [109/200], Loss: 3.026188778676442e-06\n",
      "Epoch [110/200], Loss: 3.0202913876564708e-06\n",
      "Epoch [111/200], Loss: 3.0160231290210504e-06\n",
      "Epoch [112/200], Loss: 3.0109608815109823e-06\n",
      "Epoch [113/200], Loss: 3.007518671438447e-06\n",
      "Epoch [114/200], Loss: 3.0024639272596687e-06\n",
      "Epoch [115/200], Loss: 2.998321178893093e-06\n",
      "Epoch [116/200], Loss: 2.9957898277643835e-06\n",
      "Epoch [117/200], Loss: 2.9909426757512847e-06\n",
      "Epoch [118/200], Loss: 2.986860408782377e-06\n",
      "Epoch [119/200], Loss: 2.9826535410393262e-06\n",
      "Epoch [120/200], Loss: 2.9793807243549963e-06\n",
      "Epoch [121/200], Loss: 2.976981704705395e-06\n",
      "Epoch [122/200], Loss: 2.9737066142843105e-06\n",
      "Epoch [123/200], Loss: 2.969243951156386e-06\n",
      "Epoch [124/200], Loss: 2.9667592116311425e-06\n",
      "Epoch [125/200], Loss: 2.9625989554915577e-06\n",
      "Epoch [126/200], Loss: 2.9607854230562225e-06\n",
      "Epoch [127/200], Loss: 2.9569912385341013e-06\n",
      "Epoch [128/200], Loss: 2.9546522455348168e-06\n",
      "Epoch [129/200], Loss: 2.9530608571803896e-06\n",
      "Epoch [130/200], Loss: 2.950033376691863e-06\n",
      "Epoch [131/200], Loss: 2.947247821794008e-06\n",
      "Epoch [132/200], Loss: 2.946224412880838e-06\n",
      "Epoch [133/200], Loss: 2.9432515020744177e-06\n",
      "Epoch [134/200], Loss: 2.940966851383564e-06\n",
      "Epoch [135/200], Loss: 2.9385539619397605e-06\n",
      "Epoch [136/200], Loss: 2.936389364549541e-06\n",
      "Epoch [137/200], Loss: 2.934583790192846e-06\n",
      "Epoch [138/200], Loss: 2.930529944933369e-06\n",
      "Epoch [139/200], Loss: 2.929770289483713e-06\n",
      "Epoch [140/200], Loss: 2.927002697106218e-06\n",
      "Epoch [141/200], Loss: 2.9249115414131666e-06\n",
      "Epoch [142/200], Loss: 2.922858357123914e-06\n",
      "Epoch [143/200], Loss: 2.921980467363028e-06\n",
      "Epoch [144/200], Loss: 2.920446149801137e-06\n",
      "Epoch [145/200], Loss: 2.919767439379939e-06\n",
      "Epoch [146/200], Loss: 2.915960521931993e-06\n",
      "Epoch [147/200], Loss: 2.9149421152396826e-06\n",
      "Epoch [148/200], Loss: 2.9123559670551913e-06\n",
      "Epoch [149/200], Loss: 2.9100890515110223e-06\n",
      "Epoch [150/200], Loss: 2.909833483499824e-06\n",
      "Epoch [151/200], Loss: 2.908301667048363e-06\n",
      "Epoch [152/200], Loss: 2.90677144221263e-06\n",
      "Epoch [153/200], Loss: 2.9042232654319378e-06\n",
      "Epoch [154/200], Loss: 2.904177790696849e-06\n",
      "Epoch [155/200], Loss: 2.9008106139372103e-06\n",
      "Epoch [156/200], Loss: 2.899788569266093e-06\n",
      "Epoch [157/200], Loss: 2.8987719815631863e-06\n",
      "Epoch [158/200], Loss: 2.8963563636352774e-06\n",
      "Epoch [159/200], Loss: 2.8932645363966003e-06\n",
      "Epoch [160/200], Loss: 2.8921206194354454e-06\n",
      "Epoch [161/200], Loss: 2.891504891522345e-06\n",
      "Epoch [162/200], Loss: 2.8903609745611902e-06\n",
      "Epoch [163/200], Loss: 2.8889510303997668e-06\n",
      "Epoch [164/200], Loss: 2.888437165893265e-06\n",
      "Epoch [165/200], Loss: 2.8875372208858607e-06\n",
      "Epoch [166/200], Loss: 2.885250523831928e-06\n",
      "Epoch [167/200], Loss: 2.8838451271440135e-06\n",
      "Epoch [168/200], Loss: 2.882676881199586e-06\n",
      "Epoch [169/200], Loss: 2.88176306639798e-06\n",
      "Epoch [170/200], Loss: 2.8808872230001725e-06\n",
      "Epoch [171/200], Loss: 2.8791250770154875e-06\n",
      "Epoch [172/200], Loss: 2.879365865737782e-06\n",
      "Epoch [173/200], Loss: 2.8782303616026184e-06\n",
      "Epoch [174/200], Loss: 2.8771594315912807e-06\n",
      "Epoch [175/200], Loss: 2.8750632736773696e-06\n",
      "Epoch [176/200], Loss: 2.873410039683222e-06\n",
      "Epoch [177/200], Loss: 2.8726853997795843e-06\n",
      "Epoch [178/200], Loss: 2.871437118301401e-06\n",
      "Epoch [179/200], Loss: 2.870602429538849e-06\n",
      "Epoch [180/200], Loss: 2.8710007882182254e-06\n",
      "Epoch [181/200], Loss: 2.869592890419881e-06\n",
      "Epoch [182/200], Loss: 2.8674187433352927e-06\n",
      "Epoch [183/200], Loss: 2.866404201995465e-06\n",
      "Epoch [184/200], Loss: 2.865134092644439e-06\n",
      "Epoch [185/200], Loss: 2.8646434202528326e-06\n",
      "Epoch [186/200], Loss: 2.8627796382352244e-06\n",
      "Epoch [187/200], Loss: 2.8623903745028656e-06\n",
      "Epoch [188/200], Loss: 2.8614483653655043e-06\n",
      "Epoch [189/200], Loss: 2.8607057629415067e-06\n",
      "Epoch [190/200], Loss: 2.8609042601601686e-06\n",
      "Epoch [191/200], Loss: 2.858417019524495e-06\n",
      "Epoch [192/200], Loss: 2.857915660570143e-06\n",
      "Epoch [193/200], Loss: 2.8561344151967205e-06\n",
      "Epoch [194/200], Loss: 2.8558281428558985e-06\n",
      "Epoch [195/200], Loss: 2.855159891623771e-06\n",
      "Epoch [196/200], Loss: 2.8546282919705845e-06\n",
      "Epoch [197/200], Loss: 2.852934130714857e-06\n",
      "Epoch [198/200], Loss: 2.8523331820906606e-06\n",
      "Epoch [199/200], Loss: 2.851541921700118e-06\n",
      "Epoch [200/200], Loss: 2.850036253221333e-06\n"
     ]
    }
   ],
   "source": [
    "# Define your loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define your optimizer\n",
    "model2 = Branch2()\n",
    "model2 = model2.cuda()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=1e-7)\n",
    "\n",
    "# Assuming 'inputs' is your list of inputs and 'images' is your list of target images\n",
    "# Assuming both 'inputs' and 'images' are PyTorch tensors or numpy arrays\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'inputs' and 'images' have the same length\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model2.train()  # Set the model to train mode\n",
    "    \n",
    "\n",
    "    for i in range(len(images)):\n",
    "        try:\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "            # Forward pass\n",
    "            output_image = model2(inputs[i])\n",
    "\n",
    "            loss = criterion(output_image, images[i])\n",
    "\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n",
    "    \n",
    "           \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2.state_dict(), 'branch2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.00023766126832924783\n",
      "Epoch [2/200], Loss: 0.00022810112568549812\n",
      "Epoch [3/200], Loss: 0.00021897487749811262\n",
      "Epoch [4/200], Loss: 0.00021013688819948584\n",
      "Epoch [5/200], Loss: 0.0002017416263697669\n",
      "Epoch [6/200], Loss: 0.00019367611093912274\n",
      "Epoch [7/200], Loss: 0.00018600467592477798\n",
      "Epoch [8/200], Loss: 0.0001785646309144795\n",
      "Epoch [9/200], Loss: 0.00017141838907264173\n",
      "Epoch [10/200], Loss: 0.00016455433797091246\n",
      "Epoch [11/200], Loss: 0.00015786774747539312\n",
      "Epoch [12/200], Loss: 0.0001515159965492785\n",
      "Epoch [13/200], Loss: 0.00014531683700624853\n",
      "Epoch [14/200], Loss: 0.00013940308417659253\n",
      "Epoch [15/200], Loss: 0.0001336162822553888\n",
      "Epoch [16/200], Loss: 0.00012808280007448047\n",
      "Epoch [17/200], Loss: 0.00012275901099201292\n",
      "Epoch [18/200], Loss: 0.00011756337335100397\n",
      "Epoch [19/200], Loss: 0.00011262306361459196\n",
      "Epoch [20/200], Loss: 0.0001078457135008648\n",
      "Epoch [21/200], Loss: 0.00010329403448849916\n",
      "Epoch [22/200], Loss: 9.889647481031716e-05\n",
      "Epoch [23/200], Loss: 9.467225027037784e-05\n",
      "Epoch [24/200], Loss: 9.067192149814218e-05\n",
      "Epoch [25/200], Loss: 8.679858001414686e-05\n",
      "Epoch [26/200], Loss: 8.309534314321354e-05\n",
      "Epoch [27/200], Loss: 7.957789784995839e-05\n",
      "Epoch [28/200], Loss: 7.622328121215105e-05\n",
      "Epoch [29/200], Loss: 7.303983875317499e-05\n",
      "Epoch [30/200], Loss: 6.993729039095342e-05\n",
      "Epoch [31/200], Loss: 6.698346260236576e-05\n",
      "Epoch [32/200], Loss: 6.412919174181297e-05\n",
      "Epoch [33/200], Loss: 6.132929411251098e-05\n",
      "Epoch [34/200], Loss: 5.862336547579616e-05\n",
      "Epoch [35/200], Loss: 5.600465010502376e-05\n",
      "Epoch [36/200], Loss: 5.346389298210852e-05\n",
      "Epoch [37/200], Loss: 5.099896952742711e-05\n",
      "Epoch [38/200], Loss: 4.8621659516356885e-05\n",
      "Epoch [39/200], Loss: 4.629881732398644e-05\n",
      "Epoch [40/200], Loss: 4.407660526339896e-05\n",
      "Epoch [41/200], Loss: 4.1885421524057165e-05\n",
      "Epoch [42/200], Loss: 3.979271787102334e-05\n",
      "Epoch [43/200], Loss: 3.7773170333821326e-05\n",
      "Epoch [44/200], Loss: 3.5778761230176315e-05\n",
      "Epoch [45/200], Loss: 3.3888260077219456e-05\n",
      "Epoch [46/200], Loss: 3.2068302971310914e-05\n",
      "Epoch [47/200], Loss: 3.0320419682539068e-05\n",
      "Epoch [48/200], Loss: 2.8651696993620135e-05\n",
      "Epoch [49/200], Loss: 2.7039006454288028e-05\n",
      "Epoch [50/200], Loss: 2.5510582418064587e-05\n",
      "Epoch [51/200], Loss: 2.406687599432189e-05\n",
      "Epoch [52/200], Loss: 2.268627576995641e-05\n",
      "Epoch [53/200], Loss: 2.1396130250650458e-05\n",
      "Epoch [54/200], Loss: 2.0160185158601962e-05\n",
      "Epoch [55/200], Loss: 1.900926508824341e-05\n",
      "Epoch [56/200], Loss: 1.7929725800058804e-05\n",
      "Epoch [57/200], Loss: 1.689865894149989e-05\n",
      "Epoch [58/200], Loss: 1.5943345715641044e-05\n",
      "Epoch [59/200], Loss: 1.5053354218252935e-05\n",
      "Epoch [60/200], Loss: 1.4214750990504399e-05\n",
      "Epoch [61/200], Loss: 1.3441759620036464e-05\n",
      "Epoch [62/200], Loss: 1.2738616533169989e-05\n",
      "Epoch [63/200], Loss: 1.2077503015461843e-05\n",
      "Epoch [64/200], Loss: 1.1470205208752304e-05\n",
      "Epoch [65/200], Loss: 1.0913370715570636e-05\n",
      "Epoch [66/200], Loss: 1.0394926903245505e-05\n",
      "Epoch [67/200], Loss: 9.917308489093557e-06\n",
      "Epoch [68/200], Loss: 9.477776984567754e-06\n",
      "Epoch [69/200], Loss: 9.06428977032192e-06\n",
      "Epoch [70/200], Loss: 8.686259207024705e-06\n",
      "Epoch [71/200], Loss: 8.331798198923934e-06\n",
      "Epoch [72/200], Loss: 8.001455171324778e-06\n",
      "Epoch [73/200], Loss: 7.694265150348656e-06\n",
      "Epoch [74/200], Loss: 7.402394203381846e-06\n",
      "Epoch [75/200], Loss: 7.136731710488675e-06\n",
      "Epoch [76/200], Loss: 6.879931788716931e-06\n",
      "Epoch [77/200], Loss: 6.647765530942706e-06\n",
      "Epoch [78/200], Loss: 6.421799298550468e-06\n",
      "Epoch [79/200], Loss: 6.214161658135708e-06\n",
      "Epoch [80/200], Loss: 6.01805140831857e-06\n",
      "Epoch [81/200], Loss: 5.8325144891568925e-06\n",
      "Epoch [82/200], Loss: 5.660344868374523e-06\n",
      "Epoch [83/200], Loss: 5.4993570302031e-06\n",
      "Epoch [84/200], Loss: 5.348854756448418e-06\n",
      "Epoch [85/200], Loss: 5.207447429711465e-06\n",
      "Epoch [86/200], Loss: 5.076542038295884e-06\n",
      "Epoch [87/200], Loss: 4.953825282427715e-06\n",
      "Epoch [88/200], Loss: 4.83907706438913e-06\n",
      "Epoch [89/200], Loss: 4.733032255899161e-06\n",
      "Epoch [90/200], Loss: 4.633286607713671e-06\n",
      "Epoch [91/200], Loss: 4.539722340268781e-06\n",
      "Epoch [92/200], Loss: 4.453355813893722e-06\n",
      "Epoch [93/200], Loss: 4.370743681647582e-06\n",
      "Epoch [94/200], Loss: 4.294981408747844e-06\n",
      "Epoch [95/200], Loss: 4.2225210563628934e-06\n",
      "Epoch [96/200], Loss: 4.155394890403841e-06\n",
      "Epoch [97/200], Loss: 4.092587460036157e-06\n",
      "Epoch [98/200], Loss: 4.032780452689622e-06\n",
      "Epoch [99/200], Loss: 3.976389052695595e-06\n",
      "Epoch [100/200], Loss: 3.924255906895269e-06\n",
      "Epoch [101/200], Loss: 3.875241873174673e-06\n",
      "Epoch [102/200], Loss: 3.8280368244159035e-06\n",
      "Epoch [103/200], Loss: 3.783069814744522e-06\n",
      "Epoch [104/200], Loss: 3.7410043205454713e-06\n",
      "Epoch [105/200], Loss: 3.701487003127113e-06\n",
      "Epoch [106/200], Loss: 3.6639901281887433e-06\n",
      "Epoch [107/200], Loss: 3.6286539852881106e-06\n",
      "Epoch [108/200], Loss: 3.594987674659933e-06\n",
      "Epoch [109/200], Loss: 3.5629579997475957e-06\n",
      "Epoch [110/200], Loss: 3.53148311660334e-06\n",
      "Epoch [111/200], Loss: 3.5028374441026244e-06\n",
      "Epoch [112/200], Loss: 3.475272251307615e-06\n",
      "Epoch [113/200], Loss: 3.4487241009628633e-06\n",
      "Epoch [114/200], Loss: 3.42368593919673e-06\n",
      "Epoch [115/200], Loss: 3.4001618587353732e-06\n",
      "Epoch [116/200], Loss: 3.3768892535590567e-06\n",
      "Epoch [117/200], Loss: 3.354417458467651e-06\n",
      "Epoch [118/200], Loss: 3.3330136375298025e-06\n",
      "Epoch [119/200], Loss: 3.313002707727719e-06\n",
      "Epoch [120/200], Loss: 3.2943034966592677e-06\n",
      "Epoch [121/200], Loss: 3.2746368106018053e-06\n",
      "Epoch [122/200], Loss: 3.2576811008766526e-06\n",
      "Epoch [123/200], Loss: 3.2397108498116722e-06\n",
      "Epoch [124/200], Loss: 3.2229906992142787e-06\n",
      "Epoch [125/200], Loss: 3.207947429473279e-06\n",
      "Epoch [126/200], Loss: 3.1922495509206783e-06\n",
      "Epoch [127/200], Loss: 3.177701046297443e-06\n",
      "Epoch [128/200], Loss: 3.1625920655642403e-06\n",
      "Epoch [129/200], Loss: 3.1484983082918916e-06\n",
      "Epoch [130/200], Loss: 3.1353354188468074e-06\n",
      "Epoch [131/200], Loss: 3.1222029974742327e-06\n",
      "Epoch [132/200], Loss: 3.11034636979457e-06\n",
      "Epoch [133/200], Loss: 3.0983030683273682e-06\n",
      "Epoch [134/200], Loss: 3.0871985927660717e-06\n",
      "Epoch [135/200], Loss: 3.0762143978790846e-06\n",
      "Epoch [136/200], Loss: 3.064701104449341e-06\n",
      "Epoch [137/200], Loss: 3.05454841509345e-06\n",
      "Epoch [138/200], Loss: 3.0451553811872145e-06\n",
      "Epoch [139/200], Loss: 3.034931751244585e-06\n",
      "Epoch [140/200], Loss: 3.025615342266974e-06\n",
      "Epoch [141/200], Loss: 3.0164740110194543e-06\n",
      "Epoch [142/200], Loss: 3.0079502266744385e-06\n",
      "Epoch [143/200], Loss: 2.999419393745484e-06\n",
      "Epoch [144/200], Loss: 2.9901309517299524e-06\n",
      "Epoch [145/200], Loss: 2.9825880574207986e-06\n",
      "Epoch [146/200], Loss: 2.9744787752861157e-06\n",
      "Epoch [147/200], Loss: 2.9675209134438774e-06\n",
      "Epoch [148/200], Loss: 2.959197900054278e-06\n",
      "Epoch [149/200], Loss: 2.952519253085484e-06\n",
      "Epoch [150/200], Loss: 2.945304231616319e-06\n",
      "Epoch [151/200], Loss: 2.937716999440454e-06\n",
      "Epoch [152/200], Loss: 2.931118388005416e-06\n",
      "Epoch [153/200], Loss: 2.924217596955714e-06\n",
      "Epoch [154/200], Loss: 2.9179311695770593e-06\n",
      "Epoch [155/200], Loss: 2.911393949034391e-06\n",
      "Epoch [156/200], Loss: 2.9053169328108197e-06\n",
      "Epoch [157/200], Loss: 2.899442279158393e-06\n",
      "Epoch [158/200], Loss: 2.894094450311968e-06\n",
      "Epoch [159/200], Loss: 2.8884589937661076e-06\n",
      "Epoch [160/200], Loss: 2.8828339964093175e-06\n",
      "Epoch [161/200], Loss: 2.8780266347894212e-06\n",
      "Epoch [162/200], Loss: 2.8725391985062743e-06\n",
      "Epoch [163/200], Loss: 2.867443981813267e-06\n",
      "Epoch [164/200], Loss: 2.862618885046686e-06\n",
      "Epoch [165/200], Loss: 2.857194431271637e-06\n",
      "Epoch [166/200], Loss: 2.8530637337098597e-06\n",
      "Epoch [167/200], Loss: 2.847845735232113e-06\n",
      "Epoch [168/200], Loss: 2.8427734832803253e-06\n",
      "Epoch [169/200], Loss: 2.8381182346493006e-06\n",
      "Epoch [170/200], Loss: 2.8342483346932568e-06\n",
      "Epoch [171/200], Loss: 2.8302631562837632e-06\n",
      "Epoch [172/200], Loss: 2.8259587452339474e-06\n",
      "Epoch [173/200], Loss: 2.8222461878613103e-06\n",
      "Epoch [174/200], Loss: 2.817823087752913e-06\n",
      "Epoch [175/200], Loss: 2.8139327241660794e-06\n",
      "Epoch [176/200], Loss: 2.810979140122072e-06\n",
      "Epoch [177/200], Loss: 2.8058766474714503e-06\n",
      "Epoch [178/200], Loss: 2.8019996989314677e-06\n",
      "Epoch [179/200], Loss: 2.7990977287117857e-06\n",
      "Epoch [180/200], Loss: 2.795487944240449e-06\n",
      "Epoch [181/200], Loss: 2.7913467874896014e-06\n",
      "Epoch [182/200], Loss: 2.7885232611879474e-06\n",
      "Epoch [183/200], Loss: 2.785175865938072e-06\n",
      "Epoch [184/200], Loss: 2.781401235552039e-06\n",
      "Epoch [185/200], Loss: 2.7777509785664733e-06\n",
      "Epoch [186/200], Loss: 2.7740231871575816e-06\n",
      "Epoch [187/200], Loss: 2.7706819309969433e-06\n",
      "Epoch [188/200], Loss: 2.7672379019350046e-06\n",
      "Epoch [189/200], Loss: 2.764419150480535e-06\n",
      "Epoch [190/200], Loss: 2.7610610686679138e-06\n",
      "Epoch [191/200], Loss: 2.757671381914406e-06\n",
      "Epoch [192/200], Loss: 2.754470870058867e-06\n",
      "Epoch [193/200], Loss: 2.7521948595676804e-06\n",
      "Epoch [194/200], Loss: 2.7490593765833182e-06\n",
      "Epoch [195/200], Loss: 2.746368181760772e-06\n",
      "Epoch [196/200], Loss: 2.7430928639660124e-06\n",
      "Epoch [197/200], Loss: 2.7402843443269376e-06\n",
      "Epoch [198/200], Loss: 2.7371554551791633e-06\n",
      "Epoch [199/200], Loss: 2.7343526198819745e-06\n",
      "Epoch [200/200], Loss: 2.7317507829138776e-06\n"
     ]
    }
   ],
   "source": [
    "# Define your loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define your optimizer\n",
    "model3 = Branch3()\n",
    "model3 = model3.cuda()\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=1e-7)\n",
    "\n",
    "# Assuming 'inputs' is your list of inputs and 'images' is your list of target images\n",
    "# Assuming both 'inputs' and 'images' are PyTorch tensors or numpy arrays\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'inputs' and 'images' have the same length\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model3.train()  # Set the model to train mode\n",
    "    \n",
    "\n",
    "    for i in range(len(images)):\n",
    "        try:\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "            # Forward pass\n",
    "            output_image = model3(inputs[i])\n",
    "\n",
    "            loss = criterion(output_image, images[i])\n",
    "\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n",
    "    \n",
    "           \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model3.state_dict(), 'branch3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "d2 = final.branch2.state_dict()\n",
    "for x in d2:\n",
    "    print(d2[x].shape)\n",
    "\n",
    "print(d2.keys())\n",
    "\n",
    "print(d2['0.conv.weight'][0][0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "final.branch1.state_dict()['0.conv.weight'].copy_(d1['conv1.conv.weight'])\n",
    "final.branch1.state_dict()['0.conv.weight'].copy_(d1['conv1.conv.bias'])\n",
    "final.branch1.state_dict()['0.conv.weight'].copy_(d1['conv2.conv.weight'])\n",
    "final.branch1.state_dict()['0.conv.weight'].copy_(d1['conv2.conv.bias'])\n",
    "final.branch1.state_dict()['0.conv.weight'].copy_(d1['conv3.conv.weight'])\n",
    "final.branch1.state_dict()['0.conv.weight'].copy_(d1['conv1.conv.weight'])\n",
    "\n",
    "'''\n",
    "final = MCNN()\n",
    "d1_list = ['conv1.conv.weight', 'conv1.conv.bias', 'conv2.conv.weight', 'conv2.conv.bias', 'conv3.conv.weight', 'conv3.conv.bias', 'conv4.conv.weight', 'conv4.conv.bias']\n",
    "\n",
    "d2 = ['0.conv.weight', '0.conv.bias', '2.conv.weight', '2.conv.bias', '4.conv.weight', '4.conv.bias', '5.conv.weight', '5.conv.bias']\n",
    "\n",
    "for i in range(8):\n",
    "    final.branch1.state_dict()[d2[i]].copy_(d1[d1_list[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d1 = model2.state_dict()\n",
    "for i in range(8):\n",
    "    final.branch2.state_dict()[d2[i]].copy_(d1[d1_list[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = model3.state_dict()\n",
    "for i in range(8):\n",
    "    final.branch3.state_dict()[d2[i]].copy_(d1[d1_list[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your loss function\n",
    "criterion = nn.MSELoss()\n",
    "#final = MCNN()\n",
    "final = final.cuda()\n",
    "optimizer = torch.optim.Adam(final.parameters(), lr=1e-5)\n",
    "\n",
    "# Assuming 'inputs' is your list of inputs and 'images' is your list of target images\n",
    "# Assuming both 'inputs' and 'images' are PyTorch tensors or numpy arrays\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'inputs' and 'images' have the same length\n",
    "final.train()\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "      # Set the model to train mode\n",
    "    \n",
    "\n",
    "    for i in range(len(images)):\n",
    "        try:\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "            # Forward pass\n",
    "            output_image = final(inputs[i])\n",
    "            output_image = torch.tarnspose(output_image.squeeze(),(0,1))\n",
    "            \n",
    "\n",
    "            loss = criterion(output_image, images[i])\n",
    "\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final.state_dict(), 'final_trained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MODEL = MCNN()\n",
    "MODEL.load_state_dict(torch.load('final_trained.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44926.331866833774 22863350.94904707 84.46327372291738 23867.738033919602\n",
      "Final MAE -  256.7218963819073\n",
      "Final MSE -  130647.7197088404\n",
      "Final RELATIVE MAE -  0.4826472784166707\n",
      "Final RELATIVE MSE -  136.3870744795406\n"
     ]
    }
   ],
   "source": [
    "MODEL = MODEL.cuda()\n",
    "MAE = 0\n",
    "MSE = 0\n",
    "Rel_MAE = 0\n",
    "Rel_MSE = 0\n",
    "for i in range(1,176):\n",
    "    img = cv2.imread('IMG_' + str(i) + '.jpg',0)\n",
    "    img = img.astype(np.float32, copy=False)\n",
    "    ht = img.shape[0]\n",
    "    wd = img.shape[1]\n",
    "    ht_1 = int((ht/4)*4)\n",
    "    wd_1 = int((wd/4)*4)\n",
    "    img = cv2.resize(img,(wd_1,ht_1))\n",
    "    img = img.reshape((1,1,img.shape[0],img.shape[1]))\n",
    "    v = Variable(torch.from_numpy(img).type(torch.FloatTensor))\n",
    "    v = v.to('cuda')\n",
    "    OUT = MODEL(v)\n",
    "    OUT = OUT.detach().to('cpu').numpy()\n",
    "    \n",
    "\n",
    "\n",
    "    test = Image.open('IMG_' + str(i) + '.jpg')\n",
    "    #plt.imshow(test)\n",
    "    #plt.plot()\n",
    "    density = density_map('IMG_' + str(i) + '.jpg','GT_IMG_' + str(i) + '.mat')\n",
    "    transform = transforms.ToTensor()\n",
    "    test = transform(test)\n",
    "    test = test.to('cuda')\n",
    "    MAE += abs(np.sum(OUT)- np.sum(density))\n",
    "    MSE += abs(np.sum(OUT)- np.sum(density))**2\n",
    "    Rel_MAE += (abs(np.sum(OUT)- np.sum(density))/np.sum(density))\n",
    "    Rel_MSE += (abs(np.sum(OUT)- np.sum(density))**2/np.sum(density))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(MAE,MSE,Rel_MAE,Rel_MSE)\n",
    "\n",
    "\n",
    "\n",
    "print('Final MAE - ',MAE/175)\n",
    "print('Final MSE - ',MSE/175)\n",
    "print('Final RELATIVE MAE - ',Rel_MAE/175)\n",
    "print('Final RELATIVE MSE - ',Rel_MSE/175)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
